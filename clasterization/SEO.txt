Introduction to SEO. When you explore aspects of managing websites, in addition to web design and user experience patterns, you come across the concept of search engine optimization (SEO): the science of improving a website’s visibility across results from various search engines. If a website is not prominently visible in search results, it defeats the purpose of having a website in the first place. Therefore, it is extremely important to ensure that your website is easy for search engines to access.
Search engines such as Google, Yahoo!, and Bing help users find the content they are looking for. So, it is imperative that your website is visible on the search engine results page (SERP). For example, if you have a plumbing business offering services in Toronto, your site should rank high in the search engine rankings when users search for plumbers in Toronto on Google or any other search engine. Studies related to SEO suggest that more traffic usually results in more sales; therefore, traffic is one of the key components that determines revenue. In addition to increasing revenue, SEO can help with other goals such as reaching out to a larger audience or helping people access crucial information for a greater good.
Before delving into SEO methodology, you need to have a basic understanding of the following topics: What is SEO? Benefits of SEO. Challenges in SEO. Black-hat SEO vs. White-hat SEO. On-page and Off-page SEO.
What Is SEO? SEO is a methodology used to gain traffic by making your website visible in search engine results via organic or paid techniques. The term organic means using natural ways to enhance website visibility without using a pay-per-click service. There are many ways to implement SEO, such as using relevant keywords, quality content, and optimal multimedia content on your web pages.
In the past, people implemented underhand techniques (black-hat SEO) to gain website visibility. Google Search, Bing, and Yahoo! Search are the most prominent search engines used to locate information on the Web; and with these search engines getting smarter by the day, it is imperative that you use legitimate tactics to achieve natural visibility. Therefore, SEO can also be defined as the art or science of affecting website visibility in search results, ruling out using manipulative ways to trick the search engine. Google has laid out guidelines that must be adhered to, to ensure that site owners use appropriate tactics to implement SEO.
Benefits of SEO. SEO is not a cost but an investment that imparts results in the long term. You cannot expect a miracle to happen overnight—it takes weeks or even months to reach the top-of-the-results chart. There is no fixed way to accomplish this; however, a combination of several methods can help you achieve higher rankings in a calculative (and not manipulative) way.
7.The following are several advantages of implementing SEO in your web design projects:
8. Catering to a massive audience: SEO is organic, and as your site goes up the rankings, you get more traffic. Studies related to various factors of SEO (link building, user-engaging content, and so on) indicate that sites visible in the first few pages of search engine results garner more traffic than the rest. For example, if a user wants to buy sneakers online, they usually click site links that appear on the first or second SERP without bothering to scroll through the rest; they do not want to spend too much of their valuable time and are looking for a quick resolution. Not only will traffic increase as a result of SEO, but the recommendations of users visiting your website will help you get more hits and attract new customers (a lot depends on the content or what’s on offer— you learn about that in subsequent chapters). The reach and scope you achieve by implementing SEO leads to a larger target audience.
9. Permanent results: When you use a pay-per-click approach, your site appears at the top of the SERP; however, the moment you stop paying for ads, the site will no longer be visible. On the other hand, SEO results are not dependent on ads, and with efficient maintenance, your site will appear at the top of the charts without having to advertise. You do not pay Google or advertising vendors to achieve the top spot on the SERP. (Note that at times, you may need the services of a digital marketing agency for various reasons, such as providing quality content on a regular basis or obtaining better online advertisement and exposure.)
10. Low cost of implementation: In earlier days, marketing consultants advocated the use of brochures, ads, and television to advertise products. The amount of money spent on advertising was huge. Today, creating and hosting a website is not expensive. Hiring a digital marketing expert or using the services of a professional SEO organization can be a viable option, depending on your line of business or niche. In the long run, you can stabilize your site’s position among the search results with minimum upkeep, resulting in a huge return on your investment for SEO implementation.
11. Data and analytics: Earlier, data was used for monthly, annual, and sales reports by the top brass of an organization. However, data is used as a resourceful alternative with the advent of data science. Data helps you gain insight into customer preferences, marketing trends, and much more. You can get detailed analytics that help you determine game-changing factors for your business. For example, Google Analytics and Google Search Console let you gather data that can help you understand key aspects and drawbacks related to your site’s visibility. You can also see conversions and bounce rates for the users visiting your website. For example, you can better judge the number of users actually buying something on your website vs. those who visited but did not make a transaction.
12. Staying ahead of the competition: A plethora of free and commercial enterprise-grade SEO tools (for example, Google Analytics and Google Search Console) lets you see results and reports not only for your website but also for sites owned by your competitors. Therefore, you can compare your results with your competition. You receive statistics about various aspects of your competitors’ business, such as the use of certain keywords, localization factors, and analytics. Thus, you can gain a better understanding of important competitive factors and steer clear of your competition.
13. Usability: SEO and user experience go hand-in-hand. The future of SEO leans toward creating an enhanced user experience. For example, internal links on your website that point to relevant pages and content not only help search engines crawl your site but also result in easy navigation. Ultimately, SEO implementation should focus not on search engines but on the user’s requirements and intent. Keeping this broad picture in mind, attracting users with engaging content and design is a recommended SEO trait that, in turn, aims at creating an awesome user experience. For example, Google advocates for mobile-friendly sites and ranks those sites higher, because mobile has become the de facto platform for business compared to desktop- or laptop-based sites.
14. Content marketing and branding: Users tend to surf websites that are found among higher-ranked search results. Google encourages adherence to semantics and clean planning as opposed to underhanded techniques used by spammy websites. Therefore, if your page is among the top-ranked search results, it reflects a certain trust and credibility. In addition, the “content is king” paradigm encourages fresh content and engaging users, resulting in conversion optimization. As traffic increases, so will the credibility of your website. Therefore, you can ensure that the branding of the products on your site is apt, leading to better sales. This is why small- and medium-size enterprises are more focused on SEO implementation (compared to large-scale organizations, which have the funds to advertise through several channels) using localization and other techniques that result in brand awareness.
15.Challenges in SEO. The world of SEO has its hurdles. The following are various challenges you may come across when you implement SEO in your design projects: Time constraints: As mentioned earlier, people expect an SEO implementation to deliver quick results. In reality, it takes weeks or even months to see a positive result. A true SEO consultant always adheres to legitimate tactics and does not try to fool the search engines. Using devious tactics can help you gain results, but it will not be long before you are caught, resulting in penalties. Violations can cost you dearly. Some penalties are relatively lenient: your site is not featured in the top results as a result of being downgraded. But there are also severe penalties, including delisting your site from the search results (it depends on the severity of the violation).
16. Avoiding irrelevant, spammy content: Content is king, as you will hear often in SEO training and tutorials. Well, content-driven websites usually fare well with SEO implementation, provided the content is engaging and relevant. Simply stuffing your content with keywords may not lead to good results. You need to understand user intent via their queries instead of focusing on what search engines like. For example, if your site advertises plumbing services in Toronto, then using irrelevant content such as smartphone reviews or vacations in Ibiza would be misleading. The focus of the content must not distract users. Users must receive the information they expect when they surf your website. Moreover, factors such as manipulating content, aggressive link building, and poor content may eventually prove detrimental. Search engines like fresh content. Therefore, if your content has not changed for a long time, or if you have used duplicate content from another website, your site will not appear in the top results.
17. Not including SEO while designing your website: As mentioned earlier in the chapter, SEO and user experience (UX) go hand in hand. Implementing SEO in a project after the website has been designed may lead to discouraging results. While building the website, you need to consider factors important for SEO and UX design. For example, on an e-commerce website, it is imperative that the checkout facility is clearly visible, because some users just select an item and then want to check out. If the user cannot locate the checkout facility easily, it results in inefficient navigation, leading to a poor UX. Users will not visit a website again if they have difficulty navigating it.
18. Heavy-duty sites: Cramming your website with as many features as possible or too much content can affect the UX significantly. Heavy-duty sites lead to excessive page-load times, and studies have suggested that users abandon searches if page-load times are not optimal. Less is more. Proper utilization of whitespace, efficient site architecture, and user-friendly design, along with relevant content, will streamline the UX, prompting users to return.
19.An excellent example is the Google home page, www.google.com. There is a search box in the middle of the screen; users, on visiting the page, enter search terms in the box. Most users end up doing what Google expects them to do: search for information. There are no distractions on the page. This approach works because there is no bulk or clutter to waylay users when they visit the site.
20. Defective products: Customers are the most important thing for businesses, and their feedback and recommendations are vital. If the quality of your products is not up to the mark, it is likely that users will not return, regardless of your website content. Apologies do not matter in the case of a flawed product or bad customer experience. Hence, the quality of your showcased products as well as efficient issue resolution are essential in generating user traffic to your website.
21.Black-Hat SEO vs. White-Hat SEO. Black-hat SEO can be defined as deceptive and underhand techniques used to scale up to higher rankings in search results. Most people tend to underestimate the potency of search engines in determining whether the techniques used are appropriate or deceitful. However, search engines are getting smarter by the day and can figure out whether you use improper tactics and strategies to fool them.
22.Black-hat SEO may get results in the short term, but eventually you will be penalized by the search engines. The following are a few ways these techniques are implemented:
23. Stuffing keywords into content: Using massive numbers of keywords in your content may achieve short-term gains. However, if the keywords are not relevant to your content and are placed there just to fool the search engines, then eventually your site may be penalized. Remember, SEO is all about quality, not quantity; if your content is not good enough and is packed with keywords, your site may not be visible in the top search results because you are being penalized by the search engines.
24. Link farming: Buying links is never fruitful, because the search engines will know you are manipulating them. Using inappropriate links pointing to your website is detrimental because the search engines will realize that the links have no relevance and are not authentic. For example, suppose your site offers plumbing services to people in Florida. If several links on a travel website in Toronto point to your site, it doesn’t make sense: travel services are in no way connected to plumbing services. Using such deceptive techniques can have long-term consequences and may even result in your site being delisted from search results.
25. Doorway pages and cloaking: Doorway pages are used to gain high rankings in search results in a deceitful manner. These pages contain keywords and phrases that are picked up by search-engine bots and crawlers. Once you click to access the page, you are redirected to the actual page that the site owners want you to visit. Cloaking is a mechanism that renders a page visible to search engines but not website users. This short-sighted technique can prove detrimental in the long run. Google has announced penalties for using doorways, deceptive redirects, and cloaking.
26.There is much more to black-hat SEO, which you learn about in later chapters. White-hat SEO is the opposite of black-hat SEO and is focused on using optimal, organic tactics to help your site rank higher in search results. Usually it means adhering to the guidelines provided by the search engines and playing by the rules. Factors such as quality link building, user-engaging content, and optimal performance are some of the white-hat SEO strategies used by SEO consultants to streamline accessibility and visibility in search rankings. In short, you need to focus on user intent rather than try to fool the search engines.
27.On-Page and Off-Page SEO. SEO methodology involves many factors, and no site can claim to use all of them at the same time. Basically, these factors are divided into two types: On-page SEO (or on-site SEO). Off-page SEO (or off-site SEO). 
28.On-page SEO consists of factors that are in your area of control, including your code. It includes metatags and metadescriptions, headings, title tags, internal links within the site, site maps, page-load time, semantics, and ease of navigation, to mention a few. On-page SEO generally focuses on efficient presentation of content to website users. Adhering to semantics and optimal web page structure are imperative for a site’s success: they result in a site that is systematic and organized and that provides better readability not only for users but also for indexing by search-engine crawlers.
29.Off-page SEO factors include things that are not dependent on the code or are not under your control. These include forum postings, social media marketing, social bookmarking, blogs, and RSS feeds. Off-page SEO is essential in the long term as you create a community for your content. Social networking is a key factor for creating a positive online reputation, and off-page SEO lends significant trust and credibility if you play by the rules.
30.Search Engines. A search engine is a tool that searches the Web for websites relevant to real-time queries entered by users. Depending on the search string, search engines return results, which are called search engine results page (SERPs). Usually the engines get it right and display the most relevant results. There are no fixed rules used by the search engines to display accurate results.
31.Spiders or crawlers are robots that search for and index website content. The most popular search engines are Google, Yahoo!, and Bing. Search engines can determine the most relevant web pages because the companies have developed algorithms for the search process. Nowadays, quality link building, fresh and intuitive content, and streamlined navigation are the core factors that determine which websites get priority at the top of the search results. There are other factors, such as popularity of the website, relevance of the content, and use of interactive media, although the exact logic used to obtain results is a business secret. SEO is moving toward a user experience (UX) paradigm, and search engines are constantly evolving and getting smarter by the day.
32.Evolution of Search Engines. The concept of a search engine was laid out a long time ago. In 1990, the first search engine was released. The founders named it Archie (archive without a v). The following year, Veronica and Jughead were released. 1993 saw the launch of Excite and World Wide Web Wanderer. In the same year, Aliweb and Primitive Web Search were launched. Infoseek was a venture where webmasters could submit their pages in a real-time scenario. However, the game changer was AltaVista: it was the first engine that had unlimited bandwidth and could understand natural-language queries. The following years saw the launch of WebCrawler, Yahoo! Web Directory, Lycos, LookSmart, and Inktomi.
33.Google founders Larry Page and Sergey Brin created the BackRub search engine, which focused on backlinks. Unlike other search engines, which focused on keyword relevance, the Page Rank algorithm in BackRub used backlinks to determine higher rankings in search results. Later, Page and Brin changed the name of the search engine to Google and paved the way for the search engine revolution.
34.In 1997, the search engine Ask Jeeves was launched; unlike Google, it used human editors to analyze and sort search queries. However, it depended on the principle of keyword relevance, and its business was changed from a web search engine to a question-and-answer site in 2010. Yahoo! Search depended on Inktomi, an OEM search engine, until 2002.
35.After acquiring other search utilities such as AltaVista and Overture, Yahoo! developed its own web-spider-based engine. MSN, a web portal created by Microsoft, launched MSN Search in 1999. It evolved into Live Search and was later rebranded as Bing. Figure 2-1 shows the timeline and search engines launched from 1990 to the present.
36.Apart from the big three, search engines such as Baidu (China), Yandex (Russia), and Naver (South Korea) are causing a shift in user preferences when localization factors are taken into consideration.
37.The need to do deep web searching for content that is not easily accessible has led to the evolution of initiatives such as DeepDyve and Yippy, which cater to niche interests. And with users keen on privacy, search engines such as Duck Duck Go (which avoids spammy sites and doesn’t track users’ search history) have come into prominence.
38.The future of search engines is leaning toward effective enterprise search focused on increasing productivity across organizations. Search engines are becoming more intuitive and category-based. A paradigm shift is coming, where search engines will be used not only for personal use but also for professional verticals. For example, Indeed is a job portal. These special search engines are typically known as vertical search engines and cater to a specific audience; they help users locate information that usually is not available in the results of traditional search engines. Web portals are baked-in with search engines and directory approaches and increasingly lean toward creating a satisfying user experience. Another new aspect is voice search, where search engines will become an intelligent ecosystem based on user intent. As people’s needs develop and search engines evolve, and machine learning taking data science to a new level, the future looks to be more personal and enterprise-based, which will help resolve the various associated complexities with relative ease.
39.Search Engine Processes and Components. Modern search engines perform the following processes: Web crawling. Indexing. Searching.
40.Web Crawling. Web crawlers or web spiders are internet bots that help search engines update their content or index of the web content of various websites. They visit websites on a list of URLs (also called seeds) and copy all the hyperlinks on those sites. Due to the vast amount of content available on the Web, crawlers do not usually scan everything on a web page; rather, they download portions of web pages and usually target pages that are popular, relevant, and have quality links. Some spiders normalize the URLs and store them in a predefined format to avoid duplicate content. Because SEO prioritizes content that is fresh and updated frequently, some crawlers visit pages where content is updated on a regular basis. Other crawlers are defined such that they revisit all pages regardless of changes in content. It depends on the way the algorithms are written. If a crawler is archiving websites, it preserves web pages as snapshots or cached copies.
41.Crawlers identify themselves to web servers. This identification process is required, and website administrators can provide complete or limited access by defining a robots.txt file that educates the web server about pages that can be indexed as well as pages that should not be accessed. For example, the home page of a website may be accessible for indexing, but pages involved in transactions—such as payment gateway pages—are not, because they contain sensitive information. Checkout pages also are not indexed, because they do not contain relevant keyword or phrase content, compared to category/ product pages.
42.If a server receives continuous requests, it can get caught in a spider trap. In that case, the administrators can tell the crawler’s parents to stop the loops. Administrators can also estimate which web pages are being indexed and streamline the SEO properties of those web pages.
43.Googlebot (used by Google), BingBot (used by Bing and Yahoo!), and Sphinx (an open source, free search crawler written in C++) are some of popular crawlers indexing the web for their respective search engines. Figure 2-4 shows the basic functional flow of a web crawler.
44.Indexing. Indexing methodologies vary from engine to engine. Search-engine owners do not disclose what types of algorithms are used to facilitate information retrieval using indexing. Usually, sorting is done by using forward and inverted indexes. Forward indexing involves storing a list of words for each document, following an asynchronous system-processing methodology; that is, a forward index is a list of web pages and which words appear on those web pages. On the other hand, inverted indexing involves locating documents that contain the words in a user query; an inverted index is a list of words and which web pages those words appear on. Forward and inverted indexing are used for different purposes. For example, in forward indexing, search-engine spiders crawl the Web and build a list of web pages and the words that appear on each page. But in inverted indexing, a user enters a query, and the search engine identifies web pages linked to the words in the query.
45.During indexing, search engines find web pages and collect, parse, and store data so that users can retrieve information quickly and effectively. Imagine a search engine searching the complete content of every web page without indexing—given the huge volume of data on the Web, even a simple search would take hours. Indexes help reduce the time significantly; you can retrieve information in milliseconds.
46.Forward indexing and inverted indexing are also used in conjunction. During forward indexing, you can store all the words in a document. This leads to asynchronous processing and hence avoids bottlenecks (which are an issue in inverted indexes). Then you can create an inverted index by sorting the words in the forward index, to streamline the full-text search process.
47.Information such as tags, attributes, and image alt attributes are stored during indexing. Even different media types such as graphics and video can be searchable, depending on the algorithms written for indexing purposes.
48.Search Queries. A user enters a relevant word or a string of words to get information. You can use plain text to start the retrieval process. What the user enters in the search box is called a search query. This section examines the common types of search queries: navigation, informational, and transactional.
49.Navigational Search Queries. These types of queries have predetermined results, because users already know the website they want to access. Figure 2-5 shows an example: the user has typed Yahoo in the search box and wants to access the Yahoo! website. Because the user already knows the destination to be accessed, this falls under the heading of a navigational query.
50.Informational Search Queries. Informational search queries involve finding information about a broad topic and are more generic in nature. Users generally type in real-time words to research or expand their knowledge about a topic.
51.Transactional Search Queries. In this type of query, the user’s intent is focused on a transaction, which may be generic or specific.
52.A search-engine spider visits a website and accesses the robots.txt file to learn which pages of the site are accessible. On gaining access, it sends information about the content to be indexed: this may be hypertext, headings, or title tags, depending on the markup and page content. The spider or bot then turns the information into an index: a list of terms and the web pages containing those terms
53.When users enter real-time words in a search box to retrieve information, the index containing those terms and their associated websites is accessed. Algorithms (lengthy mathematical equations) configured for ranking the results come into play, and accurate results are displayed on search engine result pages (SERPs). The algorithms determine which sites are ranked higher by assigning values to web pages depending on various factors: fresh, engaging content; localization; metadata; semantics; and fonts displaying prominence, to name a few. With search engines getting smarter by the day, the algorithms keep changing and evolving to deliver accurate results in a matter of milliseconds. Figure 2-9 shows how a search query ends up in SERPs.
54.Searches can also use filters. You can customize a search by using filters that help you get the most relevant information. You can also classify results by using filters according to the categories or media types.
55. Web Directories. Not every site is highly ranked in search results, due to heavy volume and competition on the Web. Your site also may not rank as high if your advertising budget is small compared that of the heavyweights. It is also becoming difficult for local websites to achieve higher rankings, because most of the sites are optimized to gain maximum visibility. In addition, most users use Google, Yahoo!, or Bing (or, in China, Baidu), and not every site can compete with its rivals to rank higher in those engines’ search results, given the limited options. You need to gauge the alternatives to gain better visibility and increase accessibility to your site.
56. Prior to the creation of search engines, web directories were the de facto medium for businesses to reach out to the masses. A web directory contains a collection of links to websites arranged alphabetically or categorized by niche. For example, a website for a plumbing business appears in the Plumbing category, if such a category exists. Most web directories are manually edited, meaning humans sort and enter links based on the categories. Links to your site work as backlinks and help streamline the site’s accessibility to crawlers.
57. For example, DMOZ is a comprehensive, human-edited web directory maintained by voluntary editors and reviewers. It is the default, widely used web catalogue. Some web directories are free, whereas others are commercial. Best of the Web Directory is an example of a paid directory service. A listing in a popular online directory increases your site’s visibility significantly, thereby helping you garner more traffic. With high-quality links and an increase in reliable, relevant traffic, your website may receive a fair amount of exposure. Moreover, by appearing in an optimal online directory, your website reflects credibility, which is a boon for brick-and-mortar businesses.
58. Ranking in SEO. A few years ago, there was certain predictability in understanding the factors that let to a site being ranked higher in search results. However, many professionals used black-hat SEO techniques (such as link farming and stuffing pages with keywords) to gain higher rankings. Using underhanded techniques may have elevated sites in the rankings initially, but the sites were penalized later by the search engines. The deceptive techniques were rank-specific and did not take website users or customers into consideration.
59. Over time, search engines became more efficient at judging sites based on user intent and weeding out sites that were using deceit to rank higher on search engine result pages (SERPs). Today you cannot get away with spamming the search engines— Google and the others have become adept at knowing which sites are adhering to their guidelines.
60. Studies suggest that search engines consider more than 250 factors when ranking sites. Although the exact attributes that result in better rankings are not specified (they are a business secret), the fundamentals have shifted toward an enhanced user experience (UX) and providing meaningful content. “Content is king” is an adage you may have heard a million times, but the scenario has changed: “Relevant content is king” is the new mantra and is an apt motivator toward a streamlined UX. You should focus on user intent and user satisfaction rather than design sites for the search engines.
61. SEO is an amalgam of relevance and best practices designed to help users find information related to their queries. This chapter looks at on-page, on-site, and off-page SEO factors that form the crux of SEO.
62. On-Page SEO. On-page optimization is related to factors controlled by you or your code that have an effect on your site’s rankings in search results. To create an optimal experience, you need to focus on the following page-optimization factors: Title tags. Meta keywords and meta descriptions. Headings. Engaging content. Image optimization. Interactive media. Outbound and internal links. 
63.Title Tag Optimization. In a page’s HTML code, the <title> tag gives an indication of the page content. (You can find the words used between the <title> tags by viewing the page source in Firefox; the method varies depending on the browser.) Figure 3-2 shows the location of a title on a web page as well as where it can be located in the code. Over time, depending on the links and their age, a search snippet may change dynamically for the same result. Thus there is no fixed rule for how to create a page title. The title may also vary depending on the platform—especially for responsive websites on small, medium, and large screens.
64.Do not stuff keywords into a page title, because Google may penalize your site for manipulating the natural search process. Also avoid using irrelevant phrases or a single keyword in the page title. Your page title should educate users about the page content and thus must be relevant and related to the page content. Single keywords face a lot of competition, because thousands of websites use the same keyword; it is better to use long-tail terms, which may be a mix of keywords and related phrases. Also keep in mind that each page on the website should have a unique title.
65.The best practice, according to SEO experts, is to use a phrase containing relevant words (say, 8–11 words) with at most 55–65 characters. This makes sense, because extremely long phrases will not work well on mobile devices where space is a constraint. Titles must be precise and concise, and can use a mix of uppercase and lowercase characters. Avoid commonly used titles or duplicate content, because search engines display a preference for unique titles. Google Search prefers function over form, so it is a best practice to use a simple, unique title rather than a sensational, irrelevant title. You should understand and consider user intent rather than looking at titles from a search engine point of view.
66. Meta Keywords and Meta Descriptions. Recently, Google confirmed that it doesn’t consider meta keywords and descriptions as ranking factors. Nevertheless, meta keywords and meta descriptions are cached, so it would not be a best practice to ignore them. Although they are not consequential in determining search engine results, meta descriptions can be an excellent way of advertising; they may or may not be displayed in the search results. The meta description must be unique for each page on a website, like the page title. Avoid stuffing the description with keywords, and remove all special characters. Using multiple meta keywords can have a negative influence on search engines.
67. The meta robots attribute is increasingly being used by web designers. It tells crawlers whether the page should be displayed in SERPs (index/noindex) or whether you should depend on the links on the page (follow/nofollow).
68. Heading Tags (h1, h2, h3, h4, h5, and h6). Heading tags are an important on-page factor. The <h1> (heading 1) tag is crucial and must be relevant to the topic discussed on the web page. It educates readers about the topic on that page. Instead of filling a page with clutter, it is a good practice to stick to a single topic; the heading 1 tag is imperative, because it indicates the page’s topic. Use relevant words in the heading to help users and also spiders understand the page’s content. Google adheres to text semantics and emphasizes its use for better results.
69. Avoid skipping heading levels on a web page. <h1> should be followed by <h2>, which in turn may have a <h3>, and so on. You may have multiple <h2> tags or subsequent tags, if needed. Your web page must display a systematic pattern or consistency. If the formatting or styling of the headings is not to your liking, you can use CSS styling to alter it. Include keywords, but do not repeat them in the heading. Keywords used at the beginning of a heading yield better results. Avoid spamming or using irrelevant words in headings, because doing so may have a negative effect.
70. Engaging Content. Using meaningful and pertinent content in the body section of the site is vital. Relevant content is king. The content should not be irrelevant or stuffed with keywords—the search engines may penalize you for it. However, you can use keywords or close variations of them twice or three times on a page in a logical way. The content should be informative and engage the user, encouraging them to return to check out the site regularly. It is a good practice to update the content (such as technology topics) at least every six months, because Google has a penchant for updated or fresh content. (News channel sites update their content on a daily basis. Here, we are referring to product pages or informative sites, and updating or adding content for a product or topic.) Blogs must be updated on a regular basis. Use interactive media such as images, videos, and audio files on your web pages; they are intuitive and engage users, and may make the site more popular. Always spell-check and proofread your content, because incorrect grammar or spelling errors can reflect negatively on your site.
71. In addition to having meaningful content, quantity of content matters. You cannot use a keywords 3 times in 140 characters—that is keyword stuffing. In-depth, detail-oriented, relevant content helps you space out keywords evenly. It also helps users to understand the logic of the content, especially if the topic is informative and educates the user significantly. However, do not use 2,000 words just to fill the page; low-quality content results in bad UX. Remember, less is more, because quality is more important than quantity—function over form.
72. Bounce rate reflects the number of users who visit a web page and then leave. It doesn’t matter how much time they spend on the page; it focuses on whether users leave the site after viewing just one page. Low-quality content results in higher bounce rates and will eventually affect the site’s visibility.
73. Do not copy content from another website or use boilerplate content. Google search engines have been known to penalize sites that use duplicate content. Focus on user satisfaction and not on fooling the search engines. At times there are legitimate reasons for duplicate content: for example, an e-commerce site will have the same content on different pages with different URLs due to filters such as size, color, and price. Some websites have the same content on different web pages with the prefixes HTTP and HTTPS; although the rest of the URLs are the same, the prefixes mean they are treated as separate pages. Sometimes the watered-down mobile version of a website has the same content as the desktop version, resulting in duplication. Localization may also be a factor: for example, www.google.com may appear as as www.google.co.in for India. The content may be the same, but the URLS are different. In such cases, search engines may not allocate as high a ranking, because two different URLs have the same or similar content.
74. You can resolve these issues by using either a canonical tag or a 301 direct. A 301 redirect is a permanent redirect from one URL to another that helps users reach the new address. It can also be used for “404 Page not found” errors where content has been moved to a different web page.
75. Image Optimization and Interactive Media. Earlier SEO was text-based, but this has changed significantly. You should use interactive media such as audio, video, images, and infographics to connect with your users. Use captions and alternate text for media, and build relevant content around these media. You can use a single key phrase in the alt text if it is relevant to that image. You can interchange images based on the screen size, with heavy-duty images for desktop sites and lightweight images for mobile sites. Try to limit the image file size to less than 80–90 KB for optimal page-loading time. Use PNG or JPEG image formats wherever possible, because they are robust and have more visual properties. Using thumbnails and different angles for a product can be very handy, especially on e-commerce sites.
76. Using videos explaining a product or marketing a certain entity is a good practice. Google owns YouTube, and it can be a game-changing means of branding your product. Infographics are an excellent way to provide information or create timelines with relevant content.
77. Outbound and Internal Links. Internal links are a key feature of SEO. These are links on web pages that point to another page in the site or domain. SEO-related research suggests that no page on your website should be more than three clicks from the home page, meaning all pages should be easily accessible. You can use relevant anchor text to point to different pages on your site. Breadcrumbs are an efficient way to provide site navigation using links. Having a good link structure makes it easy for search engines to crawl your entire website, and easy accessibility also leads to an awesome UX.
78. Outbound links point to another domain or site. They are a good feature for informative or niche topics. Sometimes a page includes jargon or topic-specific terms; instead of wasting time explaining supplementary information on the page, you can use URLs or anchor text as outbound links to locations that explain the information in depth. SEO experts tend to doubt the content found on Wikipedia, but it is actually an excellent source of free, relevant, detail-oriented information. For example, suppose you are explaining web servers, and you use the word server in your content. Instead of explaining what a server is, you can use the word as anchor text to link to a wiki site that explains the meaning and use of servers. Linking specific terms to wiki sites such as Wikipedia and Webopedia may boost your SEO process. Not only is doing so relevant, but it also lends a certain amount of trust and credibility to your site. You can use outbound links to social media sites or blogs to help reach out to a larger audience. Just be sure you do not link to spammy or illegal sites—doing so may negate your SEO efforts, because search engines will penalize your site. Also do not link to sites that are not relevant to the topic, because two-way linking or link farming can be detrimental.
79. On-Site SEO. Whereas on-page SEO is relevant to individual pages, on-site features affect your SEO process on the website as a whole. This section explains the following: URL optimization. Site maps. Domain trust. Localization. Mobile site optimization and responsive websites. Site-loading speed or page-load time.
80. URL Optimization. URLs play an important role in SEO, and you need to plan holistically for making your URLs as SEO-friendly as possible. Each URL should be human-readable and not consist of a bunch of special characters or numbers mixed with words. It should be meaningful and should reflect what the site is about. 
81.Otherwise the value of the content may be negated, because as ranking signals may split it across the multiple URLs. For “404 Page not found” errors, you need to use 301 redirects to guide users to a working URL for that content.
82. Using a robots.txt file helps inform search engines about pages to be ignored while crawling the site. For example, a Contact Us page or About Us page may be useful if users need to access or buy a product or need help from the customer service department.
83. However, a normal user may not find the Disclaimers page important and hardly skim such pages. So, it is essential to educate crawlers about which pages need to be indexed for SEO processes. You can also indicate broken links and 404 pages in the robots.txt file.
84. SEO experts advocate the user of a favicon on the title bar next to the URL, because it lends credibility and helps with effective branding. It helps users recognize your site and improves trustworthiness significantly. Although there are no direct benefits from favicons from a SEO perspective, they enhance usability. Bookmarks in the Google Chrome browser send out signals to the Google search engine that maps bookmarks for sites on the Web. This it is not a major factor, but it certainly helps from a user perspective.
85. Site Maps. There are two types of site maps: XML site maps, which are tailored to search engines; and HTML site maps, which are directed toward users. An XML site map contains a machine-readable list of pages on your site that you want search engines to index for SEO purposes. It contains information for crawlers such as the last update, its relevance or importance, alterations, and related data. XML site maps are domain-related and help spiders perform a deep search of web pages. For example, issues such as broken links or a lack of internal linking can be crucial factors that may result in crawlers not being able to index pages. There is no guarantee that a site map will cause crawlers to index all of your website’s pages; however, it will significantly help with accessibility, because search engines can digest such data easily.
86. An HTML site map is tailored to your website’s users and helps users locate different pages. All categories and products can be listed explicitly. It streamlines the user experience by making users familiar with your website and provides better semantics. UX is a vital aspect of SEO, so it is a good practice to include both XML and HTML site maps in your process. Make sure your XML site maps for search engines are exhaustive; on the other hand, HTML site maps should be more concise so users can navigate them more easily.
87. Domain Trust and Local Domains. Your domain can be a key ranking factor because it creates trust and credibility for site users. Studies suggest that domains registered for two years or longer were considered more trustworthy than new domains. Use the .com domain extension, because it is more common than .org and other extensions. Domain localization—catering to a specific country or city—may prove to be a game changer. For example, .co.uk caters to the United Kingdom and is more specific to users in that region and those with business links to the UK. Choosing a domain with a good reputation is helpful. If the domain has been assessed some kind of penalty, it can be detrimental to your business due to lack of credibility.
88. Using keywords in a domain name may be useful; however, given all the keywords that have already been used by websites, you may not be able to have the domain name of your choice.
89. Your domain name is crucial, because it indicates what your site is all about. Opt for a simpler, unique, relevant domain name rather than a sensational name, to help users connect with your site. You can use an online dictionary to check words related to your service or product. You can also use a combination of two or three words, such as DealOn, ScoutMob, or HomeRun. You can even think outside of the box and come up with something really creative, such as Reddit, Google, and Yelp, to name a few. Again, focus on your prospective customers and come up with something catchy and easy to spell that they can relate to.
90. Mobile Site Optimization and Responsive Websites. The digital marketing era has seen the rise of smartphones as the preferred option for online purchasing, e-commerce, and finding informative content on the Web. Designers used to create a desktop version and then remove heavy-duty elements to create a watered-down version for mobile devices. But with the advent of online marketing and social media, mobile phones and tablets have gained prominence. Studies suggest that most internet traffic comes through mobile phones and tablets—they have largely surpassed desktop websites. Even web design frameworks such as Bootstrap and Foundation use the mobile-first approach, because the target audience has undergone a major shift from desktop users to mobile users.
91. Until recently, designers created two sites: one optimized for mobiles and the other for desktops. It is essential to focus more on mobile site optimization than the desktop version. However, this can be tricky if the mobile version is a stripped-down version with fewer features and less content than the desktop site. Moreover, this means you have two URLs for the same site with similar content, so you need to use the canonical tag. In addition, a watered-down mobile site results in a pathetic UX.
92. Enter responsive web design: an excellent alternative that uses a single URL for both the mobile and desktop sites. Responsiveness is rated highly by Google. All the features and content of a desktop site are present on the mobile version, meaning there is no compromise on content display; the site is user friendly and ensures an optimal UX. The bounce rate will be lower, because users can get the same information on mobiles as well as desktops. Because there is only one URL, there is no redirect, resulting in faster page-loading times. Because Google highly recommends this approach, responsive web design is here to stay. Currently, Google marks websites as mobile-friendly in mobile searches to help its users identify which websites are likely to work best on their device.
93. Site-Loading Speed. Site- or page-loading speed is an important attribute, because Google and other search engines penalize sites that take a long time to load. An optimal page-load time leads to better conversion and improves the salability of your products. Pages that take a long time to load may frustrate users and cause negative UX, leading to higher bounce rates. Loss of internet traffic or a bad user experience can damage the site’s reputation.
94. There are several ways you can improve your page-load speed: Minifying CSS, JavaScript, and other files. Minimizing HTTP requests. Using an efficient server configuration and good bandwidth. Archiving redundant data in the database, and cleaning out trash and spam. Using fewer plug-ins and third-party utilities. Interchanging data and images, depending on the screen size. Avoiding inline styles, and keeping presentation separate from markup. Using a content delivery network (CDN).
95. Off-Page SEO. Whereas on-page SEO and on-site SEO factors are based on the elements and content on your web page or site, off-page SEO factors are external and help you rank higher in SERPs. They are not design or code related and are more like promotional concepts. This section looks at the following (see Figure 3-8): Social media. Blogging. Localization and local citations. Inbound links.
96. Social Media. Expand your reach by taking advantage of social media optimization and marketing. Social media is an amazing medium with an ever-increasing scope. You can indulge in networking and increase your connections considerably. Reaching out to the modern target audience is beneficial because users can share and promote your website. Keep your audience engaged, and share updates with them. For example, Facebook and LinkedIn can be awesome utilities that let you expand your business horizons significantly. Share updates and keep your users in the loop using Twitter. You can use the capabilities of these social media sites for branding and advertising for a fraction of the cost of traditional marketing methods such as television advertising, press releases, and Yellow Pages listings. (Chapter 11 discusses social media marketing.)
97. Blogging. Blogging is an excellent tool for achieving user engagement. You can keep users abreast of the latest trends and technologies in your niche. Informative content on blogs acts as supplementary information about your products or services. Troubleshooting steps, product-relevant content, and meaningful information are some of the elements that can be included on a blog. A plethora of blogging domains and tools can help you reach out to your audience. Inbound and relevant links from your blog to your site can boost your SEO implementation significantly.
98. Localization and Citations. Local SEO is an important off-page factor because it caters to the user’s region. It is a boon especially for small- and medium-size enterprises because it helps them connect with users in their vicinity. Google My Business allows you to list your business and gain prominence in SERPs. You can place your products or services and categorize them so that they show up when a search query is used for that category or niche in the region. Information such as working hours, updates, and contact information can be provided, leading to better accessibility.
99. Local citations are brand mentions or reviews that educate users about product robustness or attributes. Local SEO utilities such as Yelp and Foursquare are extremely helpful for understanding the pros and cons of your products or services, courtesy of user feedback or input. Reviews help you establish a connection with users and understand their viewpoint and concerns related to your business. Increasing interaction with your users will help streamline your business in the long run.
100. Inbound Links. Inbound links are links from other domains pointing toward your website. Links from domains with high page rank and authority are preferable and lend more credibility than links from domains with low authority or low page rank. The number of domains that link to your website can be a crucial factor. Studies suggest that links from several different domains to your site can boost your SEO implementation. However, you should not indulge in link farming or use underhanded techniques, which may result in a penalty. There should not be too many links from a single domain, because this is an indication of spamming and can have negative consequences. Referral links from blogs, social media sites, and news aggregators are handy, provided they are relevant and contextual. Inbound links from another domain’s home page or key pages are more useful than links from a sidebar or an insignificant page location. Google recommends getting links from domains with high-quality content. Forums, reviews, and comments can contain links pointing to your website and enhance your site presence, provided they are not irrelevant. Backlinks from social bookmarking sites (such as Reddit) and web directories (such as DMOZ) can affect visibility positively.
101. Obstacles in SEO. There are certain blips you come across when implementing SEO in projects. These hurdles hinder the SEO workflow significantly and can affect your website’s visibility. SEO associates tend to tweak websites for search engines and forget to focus on the user experience. An appropriate approach is to design the website for users (user-centric) and then tweak the website for search engines. There are several botlenecks that we tend to overlook when we are implementing SEO. These bottlenecks can be game-changers that reduce your site's visibility in SERPs. In this chapter, we will discuss the obstacles in SEO and include suggestions to overcome these drawbacks.
102. Black-Hat SEO. Despite knowing that black-hat SEO will result in penalties, some SEO experts resort to underhanded techniques. Link farming, cloaking, keyword stuffing, irrelevant content, and spamming are black-hat techniques that are still in use. The results may seem positive, but eventually Google and other search engines realize that they are being duped, resulting in a penalty.
103. Let’s consider the cloaking black-hat SEO technique. It is akin to spamdexing, where the content presented to search engine crawlers is different than the content presented to human users. It is a deceitful way of achieving higher rankings: the content delivered is different depending on IP addresses or HTTP headers. This manipulative technique tries to trick search engines into believing that the content is the same as what users see.
104. Another black-hat SEO technique is link farming, where sites exchange reciprocal links to boost their rankings by fooling the search engines. It is different from link building, which is an organic way of boosting rankings. Because search engines such as Google and Bing rank sites based on their popularity and inbound links, some SEO consultants used to implement link farming to get links from hundreds of websites that were not even slightly related to the target site. Some SEO consultants also had link farms that used devious ways of exchanging links with other sites, something like a link-exchange program.
105. For example, suppose a site is devoted to troubleshooting Windows OS issues. If inbound links come from sites such as Stack Overflow, authority sites, and relevant blogs, then they are legitimate. However, if such a site receives inbound links from travel and tourism sites offering vacation packages in Miami or from sites offering plumbing solutions in Ibiza, then there is no relevance—the sites have no connection. Therefore,such sites use link farming and are deceitful because they just want to boost their rankings using these underhanded techniques.
106. Instead of fooling the search engines, it is better to use white-hat SEO techniques that are beneficial in the long run. Quality link building, using social media appropriately, and engaging users with solid content are some of the white-hat SEO techniques. It may take weeks or even months for the results to show, but white-hat techniques are the norms that SEO experts must follow to gain visibility in SERPs.
107. Irrelevant Content. Content is king. However, if you use duplicate content or inappropriate methods such as keyword stuffing in your content, you are bound to be penalized. Content must be relevant and must engage users. Fresh content is also an essential factor, because the search engines show an affinity for fresh, quality content. Write content for users, and then tweak it to optimize it for the search engines.
108. Targeting the Wrong Audience. Your SEO implementation must be optimized for an appropriate target audience. If you do not target the right audience, your efforts will be wasted. For example, gaming consoles, portable music players, and MP3 gadgets are appealing to youth, whereas long-term retirement plans are better suited for middle-aged users.
109. Competition. Small- and medium-sized businesses do not have a huge budget for advertising their products and services. Therefore, they need to ensure that they do not try the same approaches as large-scale enterprises that have a panel of SEO experts, funding, and expensive advertising methods. You should also avoid using metadata and content similar to that of large-scale enterprises, because doing so will hinder your SEO process.
110. Using keywords prevalent in the web pages of enterprise-grade organizations is detrimental because small enterprises do not have the budget, web presence, and reach for mass-scale advertising to outperform large adversaries. You can use Google My Business and other Google tools (as well as third-party enhancements) to gain visibility. You can also use keywords that have less competition and target a niche market. You may see your website gain prominence on SERPs using such low-competition keywords.
111. Overlooking Social Media as a Medium. Social media marketing is no longer an optional method. It is mandatory to take advantage of the scope and reach of social media to create awareness of your brand. Many organizations neglect social media, and this limits exposure to your site. This doesn’t mean you should over-optimize social media strategies by using every social media app in the market. You also have to be relevant.
112. For example, you can advertise by using a concise video about your product or service on YouTube. You can tweet about your latest product update on Twitter or write a blog on WordPress that is relevant to your product. Users love to read fresh and engaging content, and so do the search engines. You can also use backlinks to your site and outbound links to relevant sites (such as linking a term to Wikipedia), which will benefit users looking for informative content.
113. Ignoring UX for Your Website. Your site may have lots of jazzy features, but if your users cannot navigate easily or find it difficult to access content, then the result may be a shabby UX. For example, the Add To Cart button on an e-commerce website must be easily accessible to users. UX is a crucial factor for search engines because they like sites that are popular and have a high degree of usability.
114. Missing XML and HTML Sitemaps. XML and HTML sitemaps are designed for search engines and users, respectively. Your site may have the latest updates and game-changing features, but if the search engines are unable to crawl and map your site, that is detrimental to your SEO workflow. You must submit XML and HTML sitemaps to the search engines, so your deep web pages can be crawled more easily. (Chapter 6 is dedicated to sitemaps.)
115. Slow Page-Load Time. Slow page-load time is a deterrent to SEO processes. Code-heavy pages, uncompressed and unoptimized HTML, images, external embedded media, extensive use of Flash, and JavaScript result in slow page-loading times. There can be other factors, too, that may result in high page-load times: using server-centric dynamic scripts, non-optimal web hosting, and lack of required bandwidth. These factors negatively affect your SEO processes and are major hindrances resulting in lower rankings. For example, SEO experts recommend 2 to 3 seconds as the optimal loading time for product pages on e-commerce sites after analyzing statistics from research and analytics.
116. Moreover, surveys and studies related to site speed infer that users tend to abandon a site that isn’t loaded within 3 to 4 seconds. This also represents a shabby user experience, resulting in lower conversions and sales.
117. Using Flash on Your Site. In the early days of the web, Flash was an awesome resource that helped site build intuitive modules and impressive page elements. However, with the advent of HTML5, this cutting-edge utility has taken a backseat. Moreover, as the mobile and tablet market has become a dominant force, the use of Flash is considered redundant, because Flash was tailored for desktop users.
118. Flash is more prone to malicious malware hacking, from a security point of view. Its non-scalable features mean you cannot minimize or expand it or set a viewport for it. Everything you can do in Flash can be done more quickly and easily in HTML5. You can also use the latest web design frameworks to build interactive sites, thereby relegating Flash to the sidelines. Google and other search engines cannot read Flash, although recently, Google claims it can index the text in Flash files. Considering the pitfalls associated with Flash, you should use HTML5 to design and develop interactive sites with enhanced animation and special effects.
119. AMP. Accelerated Mobile Pages (AMP) is a Google project aimed at the mobile web. Akin to the concept of Facebook’s Instant Articles and Apple’s Apple News, AMP pages will change the way people perceive the mobile web. AMP pages are web-based, meaning they are rendered in a browser. They are independent documents that are sourced from your web server. Optionally, you can store AMP documents in a CDN cache to render them more quickly.
120. AMP pages are made up of the following modules: AMP HTML. AMP Runtime. AMP Cache
121. While responsive websites face issues such as rendering heavy-duty desktop content on a mobile website, JavaScript bloat, and sluggish speed on the mobile platform, AMP pages are designed for the mobile platform and help users view site pages efficiently across various mobile and tablet sizes. JavaScript is baked-in for AMP Runtime, which manages the loading of AMP modules along with features such as runtime validation for AMP HTML. It defines the priority for resource loading, thereby resulting in an optimal page-loading experience. AMP HTML documents can be stored on your server, and you can use your own CDN; but you can also take advantage of the benefits of using Google’s CDN, which streamlines the SEO processes built around these pages. When you search a page on Google, you see the results on desktop browsers. However, on the mobile platform, there is a high probability that Google will direct you to AMP pages rather than regular pages, because AMP load instantaneously and the Runtime streamlines the utilization of available resources.
122. Unlike Facebook’s Instant Articles or Apple’s Apple News, AMP pages (although backed by Google) are portal-agnostic and open-source. They are supposed to be built-in with ads and analytics support. AMP are accessible from any portal: Google Search, Pinterest, or anywhere online. In summary, AMP pages are a lucrative alternative to heavy-duty websites; they display information quickly and render content effectively without the bulk or clutter.
123.  Client-side JavaScript , extensively used in single-page applications, helps you build dynamic and highly intuitive websites. However, search engines cannot parse JavaScriptgenerated content efficiently. Currently, only Google’s search engine is able to understand JavaScript, albeit at a more basic level. With frameworks such as Angular and Ember being used to build dynamic web pages, search engines will have to develop the ability to understand complex JavaScript; so far, this ability is evolving. There are a few workarounds you can implement to tackle JavaScript accessibility issues. Suppose your browser is an old version and cannot parse the latest features and functions of JavaScript. You can use fallback code (also called polyfills ) to replicate the content of JavaScript-based web applications. Using fallback code is an option for tackling the JavaScript issue using server-side rendering. However, it requires a lot of effort and costs more because you must develop code for all the features on the website; plus there is heavy code maintenance. It also makes your pages code-heavy, resulting in higher page-loading times.
